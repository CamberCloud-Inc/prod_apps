{
    "name": "datasync-data-sync",
    "title": "Automated File Synchronization & Validation",
    "description": "System operation pipeline for automated data synchronization, checksum validation, and data integrity verification. Handles large-scale data transfer, validation workflows, and checkpoint-based automation in data processing facilities. Samplesheet format: `sample,fastq_1,fastq_2`",
    "version": "1.0.0",
    "engineType": "NEXTFLOW",
    "command": "nextflow run nf-core/datasync --input ${input} --outdir ${outdir} --sync ${sync} -r ${revision}",
    "imageUrl": "https://raw.githubusercontent.com/nf-core/datasync/master/docs/images/nf-core-datasync_logo_light.png",
    "content": "<h1 class=\"heading-node\">Data Sync: Automated File Synchronization & Validation</h1><img src=\"https://raw.githubusercontent.com/nf-core/datasync/master/docs/images/nf-core-datasync_logo_light.png\" alt=\"nf-core/datasync logo\" title=\"nf-core/datasync\" width=\"400\" height=\"200\"><p class=\"text-node\"><strong>Enterprise-Grade Data Management for Research Facilities</strong></p><p class=\"text-node\">The nf-core/datasync pipeline is a specialized system operation tool designed for large-scale data processing facilities. Originally developed by Alexander Peltzer, this pipeline automates complex data synchronization, validation, and management workflows that are essential for modern genomics and bioinformatics infrastructure.</p><h2 class=\"heading-node\">The Data Management Challenge</h2><p class=\"text-node\">Research facilities handling high-throughput sequencing and large datasets face critical challenges:</p><ul class=\"list-node\"><li><p class=\"text-node\"><strong>Data Transfer at Scale:</strong> Moving terabytes of sequencing data between storage systems</p></li><li><p class=\"text-node\"><strong>Integrity Verification:</strong> Ensuring files are not corrupted during transfer</p></li><li><p class=\"text-node\"><strong>Workflow Coordination:</strong> Synchronizing downstream analysis with upstream data generation</p></li><li><p class=\"text-node\"><strong>Checkpoint Management:</strong> Tracking completion of multi-stage data processing</p></li><li><p class=\"text-node\"><strong>Audit Trails:</strong> Maintaining comprehensive records of data movements</p></li><li><p class=\"text-node\"><strong>Error Recovery:</strong> Handling partial transfers and resuming interrupted operations</p></li></ul><p class=\"text-node\"><strong>The Solution:</strong> This pipeline provides automated, reproducible data synchronization with built-in validation, checkpoint tracking, and comprehensive reporting.</p><h2 class=\"heading-node\">Core Features</h2><h3 class=\"heading-node\">1. Data Synchronization with Integrity Verification</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>Checksum Generation:</strong> Calculate SHA256 or MD5 checksums for all files</p></li><li><p class=\"text-node\"><strong>Transfer Verification:</strong> Validate data integrity after synchronization</p></li><li><p class=\"text-node\"><strong>Configurable Backend:</strong> Choose between SHA256 (more secure) or MD5 (faster)</p></li><li><p class=\"text-node\"><strong>Batch Processing:</strong> Handle thousands of files in parallel</p></li></ul><h3 class=\"heading-node\">2. Flexible File Selection</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>Include/Exclude Rules:</strong> YAML-based configuration for folder selection</p></li><li><p class=\"text-node\"><strong>Pattern Matching:</strong> Use wildcards and regex for file filtering</p></li><li><p class=\"text-node\"><strong>Subfolder Control:</strong> Recursively sync entire directory trees or specific levels</p></li><li><p class=\"text-node\"><strong>Selective Synchronization:</strong> Skip already-transferred or unchanged files</p></li></ul><h3 class=\"heading-node\">3. Checkpoint-Based Workflow Automation</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>Completion Markers:</strong> Support for checkpoint files (e.g., DEMUX_DONE, ANALYSIS_COMPLETE)</p></li><li><p class=\"text-node\"><strong>Upstream Dependency:</strong> Wait for checkpoint signals before starting sync</p></li><li><p class=\"text-node\"><strong>Downstream Triggering:</strong> Create checkpoint files to signal completion</p></li><li><p class=\"text-node\"><strong>Conditional Execution:</strong> Only process folders with specific markers</p></li></ul><h3 class=\"heading-node\">4. Validation Workflows</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>Checksum Matching:</strong> Verify transferred files match source checksums</p></li><li><p class=\"text-node\"><strong>File Existence Checks:</strong> Confirm files present in both source and target</p></li><li><p class=\"text-node\"><strong>Corruption Detection:</strong> Identify damaged or incomplete transfers</p></li><li><p class=\"text-node\"><strong>Comparison Reports:</strong> Generate detailed validation summaries</p></li></ul><h3 class=\"heading-node\">5. Comprehensive Reporting</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>MultiQC Custom Content:</strong> Interactive HTML reports</p></li><li><p class=\"text-node\"><strong>Transfer Statistics:</strong> File counts, sizes, transfer rates</p></li><li><p class=\"text-node\"><strong>Validation Results:</strong> Success/failure status for each file</p></li><li><p class=\"text-node\"><strong>Audit Logs:</strong> Complete record of all operations</p></li></ul><h2 class=\"heading-node\">Input Requirements</h2><h3 class=\"heading-node\">Samplesheet Format</h3><p class=\"text-node\">Create a CSV file listing files or directories to synchronize:</p><pre class=\"block-node\"><code>sample,fastq_1,fastq_2\nrun_2024_01_15,/source/path/run_2024_01_15_R1.fastq.gz,/source/path/run_2024_01_15_R2.fastq.gz\nrun_2024_01_16,/source/path/run_2024_01_16_R1.fastq.gz,/source/path/run_2024_01_16_R2.fastq.gz\nanalysis_results,/source/path/analysis_complete.tar.gz,</code></pre><p class=\"text-node\"><strong>Column Details:</strong></p><ul class=\"list-node\"><li><p class=\"text-node\"><code class=\"inline\" spellcheck=\"false\">sample</code>: Unique identifier for this sync operation (run ID, dataset name)</p></li><li><p class=\"text-node\"><code class=\"inline\" spellcheck=\"false\">fastq_1</code>: Path to primary file or directory to synchronize</p></li><li><p class=\"text-node\"><code class=\"inline\" spellcheck=\"false\">fastq_2</code>: Optional secondary file or directory (leave empty if not needed)</p></li></ul><p class=\"text-node\"><strong>Important Notes:</strong></p><ul class=\"list-node\"><li><p class=\"text-node\">Supports both individual files and entire directories</p></li><li><p class=\"text-node\">Paths can be absolute or relative</p></li><li><p class=\"text-node\">Compressed files (.gz, .tar.gz, .zip) are handled automatically</p></li><li><p class=\"text-node\">Spaces in sample names converted to underscores</p></li></ul><h3 class=\"heading-node\">Configuration Files (Optional)</h3><p class=\"text-node\"><strong>YAML Configuration for Include/Exclude Rules:</strong></p><pre class=\"block-node\"><code>include:\n  - \"*.fastq.gz\"\n  - \"*.bam\"\n  - \"analysis_results/*\"\nexclude:\n  - \"*_tmp_*\"\n  - \"*.log\"\n  - \".temp/\"</code></pre><h2 class=\"heading-node\">Common Use Cases</h2><h3 class=\"heading-node\">1. Sequencing Facility Data Transfer</h3><p class=\"text-node\"><strong>Scenario:</strong> Automated transfer of sequencing runs from instrument to analysis server</p><ul class=\"list-node\"><li><p class=\"text-node\">Monitor instrument output directory for DEMUX_DONE checkpoint</p></li><li><p class=\"text-node\">Transfer FASTQ files with SHA256 verification</p></li><li><p class=\"text-node\">Create TRANSFER_COMPLETE marker for downstream pipelines</p></li><li><p class=\"text-node\">Generate audit report for facility records</p></li></ul><h3 class=\"heading-node\">2. Data Migration Between Storage Systems</h3><p class=\"text-node\"><strong>Scenario:</strong> Moving archived data to new storage infrastructure</p><ul class=\"list-node\"><li><p class=\"text-node\">Batch process thousands of BAM files</p></li><li><p class=\"text-node\">Verify checksums during transfer</p></li><li><p class=\"text-node\">Validate file existence in both locations</p></li><li><p class=\"text-node\">Track migration progress with detailed reports</p></li></ul><h3 class=\"heading-node\">3. Backup Validation</h3><p class=\"text-node\"><strong>Scenario:</strong> Verify integrity of archived research data</p><ul class=\"list-node\"><li><p class=\"text-node\">Compare source and backup checksums</p></li><li><p class=\"text-node\">Identify corrupted or missing files</p></li><li><p class=\"text-node\">Generate validation certificates for compliance</p></li><li><p class=\"text-node\">Schedule periodic integrity checks</p></li></ul><h3 class=\"heading-node\">4. Multi-Site Data Sharing</h3><p class=\"text-node\"><strong>Scenario:</strong> Sharing datasets between collaborating research facilities</p><ul class=\"list-node\"><li><p class=\"text-node\">Package analysis results for transfer</p></li><li><p class=\"text-node\">Generate checksums for remote verification</p></li><li><p class=\"text-node\">Document transfer with comprehensive logs</p></li><li><p class=\"text-node\">Provide validation tools for recipient site</p></li></ul><h3 class=\"heading-node\">5. Pipeline Workflow Coordination</h3><p class=\"text-node\"><strong>Scenario:</strong> Coordinate multi-stage analysis workflows</p><ul class=\"list-node\"><li><p class=\"text-node\">Wait for upstream process completion (ALIGN_DONE)</p></li><li><p class=\"text-node\">Sync intermediate results to shared storage</p></li><li><p class=\"text-node\">Signal downstream processes (READY_FOR_VARIANT_CALLING)</p></li><li><p class=\"text-node\">Maintain workflow state with checkpoint files</p></li></ul><h2 class=\"heading-node\">Comprehensive Output Files</h2><h3 class=\"heading-node\">Synchronization Results</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>Checksums:</strong> SHA256 or MD5 files for all transferred data</p></li><li><p class=\"text-node\"><strong>Transfer Logs:</strong> Detailed record of each sync operation</p></li><li><p class=\"text-node\"><strong>File Manifests:</strong> Complete list of synchronized files</p></li><li><p class=\"text-node\"><strong>Checkpoint Files:</strong> Completion markers (e.g., SYNC_DONE)</p></li></ul><h3 class=\"heading-node\">Validation Reports</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>Checksum Verification:</strong> Match/mismatch status for each file</p></li><li><p class=\"text-node\"><strong>File Comparison:</strong> Source vs. target file lists</p></li><li><p class=\"text-node\"><strong>Error Summary:</strong> Failed transfers or validation failures</p></li><li><p class=\"text-node\"><strong>Statistics:</strong> Total files, sizes, success rates</p></li></ul><h3 class=\"heading-node\">Quality Control</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>MultiQC Report:</strong> Interactive HTML dashboard</p></li><li><p class=\"text-node\"><strong>Custom Metrics:</strong> Transfer rates, file counts, error rates</p></li><li><p class=\"text-node\"><strong>Timeline Visualization:</strong> Sync operation progress over time</p></li></ul><h3 class=\"heading-node\">Audit Trail</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>Pipeline Info:</strong> Execution details and parameters</p></li><li><p class=\"text-node\"><strong>Software Versions:</strong> Tool versions for reproducibility</p></li><li><p class=\"text-node\"><strong>Execution Logs:</strong> Complete command history</p></li><li><p class=\"text-node\"><strong>Timestamp Records:</strong> Start/end times for all operations</p></li></ul><h2 class=\"heading-node\">Technical Implementation</h2><ul class=\"list-node\"><li><p class=\"text-node\"><strong>Pipeline Framework:</strong> Built with Nextflow DSL2</p></li><li><p class=\"text-node\"><strong>Containers:</strong> Docker/Singularity for portability</p></li><li><p class=\"text-node\"><strong>Parallel Processing:</strong> Simultaneous handling of multiple files</p></li><li><p class=\"text-node\"><strong>Resume Capability:</strong> Automatic recovery from interruptions</p></li><li><p class=\"text-node\"><strong>Resource Management:</strong> Configurable CPU and memory allocation</p></li><li><p class=\"text-node\"><strong>Cloud Compatible:</strong> Works with AWS S3, GCP Storage, Azure Blob</p></li></ul><h2 class=\"heading-node\">Key Parameters</h2><ul class=\"list-node\"><li><p class=\"text-node\"><code class=\"inline\" spellcheck=\"false\">--sync</code>: Enable data synchronization workflow</p></li><li><p class=\"text-node\"><code class=\"inline\" spellcheck=\"false\">--sync_backend</code>: Choose checksum algorithm (sha256 or md5)</p></li><li><p class=\"text-node\"><code class=\"inline\" spellcheck=\"false\">--sync_done</code>: Create SYNC_DONE checkpoint file on completion</p></li><li><p class=\"text-node\"><code class=\"inline\" spellcheck=\"false\">--validate</code>: Run validation workflow to verify checksums</p></li><li><p class=\"text-node\"><code class=\"inline\" spellcheck=\"false\">--input</code>: Path to samplesheet CSV</p></li><li><p class=\"text-node\"><code class=\"inline\" spellcheck=\"false\">--outdir</code>: Output directory for reports and logs</p></li></ul><h2 class=\"heading-node\">Best Practices</h2><h3 class=\"heading-node\">Checksum Selection</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>SHA256:</strong> More secure, recommended for long-term archives</p></li><li><p class=\"text-node\"><strong>MD5:</strong> Faster computation, suitable for routine transfers</p></li><li><p class=\"text-node\"><strong>Consistency:</strong> Use same algorithm throughout facility</p></li></ul><h3 class=\"heading-node\">Performance Optimization</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>Batch Size:</strong> Group similar files for efficient processing</p></li><li><p class=\"text-node\"><strong>Network:</strong> Run during off-peak hours for large transfers</p></li><li><p class=\"text-node\"><strong>Resources:</strong> Allocate sufficient CPU for parallel operations</p></li><li><p class=\"text-node\"><strong>Resume:</strong> Use Nextflow resume for interrupted runs</p></li></ul><h3 class=\"heading-node\">Workflow Integration</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>Checkpoint Files:</strong> Use standardized marker names across facility</p></li><li><p class=\"text-node\"><strong>Automation:</strong> Schedule regular sync operations with cron/scheduler</p></li><li><p class=\"text-node\"><strong>Monitoring:</strong> Set up alerts for validation failures</p></li><li><p class=\"text-node\"><strong>Documentation:</strong> Maintain samplesheet templates for common operations</p></li></ul><h2 class=\"heading-node\">Development Status</h2><blockquote class=\"block-node\"><p class=\"text-node\"><strong>Note:</strong> This pipeline is under active development. While core functionality is stable, additional features are being added. Check the GitHub repository for latest updates and feature roadmap.</p></blockquote><h2 class=\"heading-node\">Community and Support</h2><ul class=\"list-node\"><li><p class=\"text-node\"><strong>nf-core Slack:</strong> #datasync channel for questions and discussion</p></li><li><p class=\"text-node\"><strong>GitHub Issues:</strong> Report bugs or request features</p></li><li><p class=\"text-node\"><strong>Documentation:</strong> Comprehensive guides at nf-co.re/datasync</p></li><li><p class=\"text-node\"><strong>Community:</strong> Contribute improvements via pull requests</p></li></ul><p class=\"text-node\"><strong>Perfect for:</strong> Data management \u2022 Sequencing facilities \u2022 Storage migration \u2022 Backup validation \u2022 Workflow automation \u2022 Quality assurance \u2022 Audit compliance \u2022 Research infrastructure</p><p class=\"text-node\">Built with <a class=\"link\" href=\"https://www.nextflow.io/\" target=\"_blank\">Nextflow</a> \u2022 Powered by <a class=\"link\" href=\"https://nf-co.re/datasync\" target=\"_blank\">nf-core/datasync</a> \u2022 Containerized execution \u2022 Enterprise-ready</p>",
    "spec": [
        {
            "type": "Stash File",
            "label": "Input Samplesheet",
            "name": "input",
            "description": "CSV samplesheet with columns: sample, fastq_1, fastq_2. List files or directories to synchronize with optional paired entries.",
            "required": true,
            "restrictions": {
                "allow_files": true,
                "allowed_file_types": [
                    ".csv",
                    ".tsv"
                ]
            }
        },
        {
            "type": "Stash File",
            "label": "Output Directory",
            "name": "outdir",
            "description": "Directory where synchronization results, validation reports, and audit logs will be saved.",
            "defaultValue": "./datasync-results",
            "required": true,
            "restrictions": {
                "allow_files": false,
                "allow_folders": true
            }
        },
        {
            "type": "Switch",
            "label": "Enable Sync",
            "name": "sync",
            "description": "Enable data synchronization workflow with checksum verification.",
            "defaultValue": true,
            "required": false
        },
        {
            "type": "Select",
            "label": "Sync Backend",
            "name": "sync_backend",
            "description": "Checksum algorithm for data integrity verification. SHA256 more secure, MD5 faster.",
            "defaultValue": "sha256",
            "required": false,
            "options": [
                {
                    "label": "SHA256 (Recommended - more secure)",
                    "value": "sha256"
                },
                {
                    "label": "MD5 (Faster computation)",
                    "value": "md5"
                }
            ]
        },
        {
            "type": "Switch",
            "label": "Create Sync Done File",
            "name": "sync_done",
            "description": "Create a SYNC_DONE checkpoint file when synchronization completes successfully. Useful for workflow automation.",
            "defaultValue": false,
            "required": false
        },
        {
            "type": "Input",
            "label": "Pipeline Revision",
            "name": "revision",
            "description": "Version or revision of the nf-core/datasync pipeline to use.",
            "defaultValue": "dev",
            "required": true
        }
    ],
    "jobConfig": [
        {
            "type": "Select",
            "label": "System Size",
            "name": "system_size",
            "description": "Select compute resources based on data volume. SMALL suitable for most sync operations.",
            "options": [
                {
                    "label": "SMALL (16 CPUs, 60GB RAM) - Standard sync operations",
                    "value": "small",
                    "mapValue": {
                        "nodeSize": "SMALL",
                        "numNodes": 1,
                        "withGpu": false
                    }
                },
                {
                    "label": "MEDIUM (32 CPUs, 180GB RAM) - Large file batches",
                    "value": "medium",
                    "mapValue": {
                        "nodeSize": "MEDIUM",
                        "numNodes": 1,
                        "withGpu": false
                    }
                },
                {
                    "label": "LARGE (64 CPUs, 360GB RAM) - Massive datasets",
                    "value": "large",
                    "mapValue": {
                        "nodeSize": "LARGE",
                        "numNodes": 1,
                        "withGpu": false
                    }
                }
            ],
            "defaultValue": "small"
        }
    ],
    "tags": [
        {
            "id": "fb1ee42a-629a-4d99-9e7f-1da3c5f7a691",
            "name": "biology",
            "type": "field"
        },
        {
            "id": "6e4e74a4-560e-4674-95c3-60fe895b8c91",
            "name": "genomics",
            "type": "subfield"
        },
        {
            "id": "831477f4-f572-4e27-af83-037661489800",
            "name": "analysis",
            "type": "task"
        }
    ]
}