{
  "name": "deepmodeloptim-ml-optimization",
  "title": "STIMULUS",
  "description": "Automated deep learning model testing and training data optimization pipeline. Systematically transforms data through preprocessing pipelines, tunes hyperparameters, performs sanity checks, and trains minimal models to identify optimal dataset-architecture combinations for genomics and bioinformatics applications. Samplesheet format: `gene_1:input:float,gene_2:input:float,age:meta:int,disease_status:label:binary`",
  "version": "1.3.1",
  "engineType": "NEXTFLOW",
  "command": "nextflow run nf-core/deepmodeloptim --input ${input} --outdir ${outdir} -r dev -profile docker",
  "imageUrl": "https://raw.githubusercontent.com/nf-core/deepmodeloptim/master/docs/images/nf-core-deepmodeloptim_logo_light.png",
  "content": "<h1 class=\"heading-node\">\ud83e\udde0 Deep Learning Model Optimization: STIMULUS</h1><p class=\"text-node\"><strong>Stochastic Testing and Input Manipulation for Unbiased Learning Systems</strong></p><p class=\"text-node\">Optimizing deep learning models for biological data is challenging: researchers must define preprocessing pipelines, select architectures, tune hyperparameters, and iterate repeatedly. The <strong>nf-core/deepmodeloptim</strong> pipeline automates this entire workflow, systematically testing preprocessing strategies and architectures to identify the optimal configuration for your specific task.</p><img src=\"https://raw.githubusercontent.com/nf-core/deepmodeloptim/dev/docs/images/nf-core-deepmodeloptim_metro_map.png\" alt=\"deepmodeloptim workflow\" width=\"600\" height=\"400\"><h2 class=\"heading-node\">\ud83c\udfaf The Deep Learning Challenge</h2><p class=\"text-node\"><strong>The Problem:</strong> Most deep learning performance is driven by the training data, yet the connection between data preprocessing and model performance is often overlooked or tested manually. Researchers face:</p><ul class=\"list-node\"><li><p class=\"text-node\">\ud83d\udd04 <strong>Manual iteration:</strong> Testing preprocessing steps one-by-one</p></li><li><p class=\"text-node\">\ud83c\udfb2 <strong>Arbitrary choices:</strong> Guessing which normalization or transformation works best</p></li><li><p class=\"text-node\">\u2699\ufe0f <strong>Hyperparameter tuning:</strong> Finding optimal architecture parameters for each data variant</p></li><li><p class=\"text-node\">\ud83d\udcca <strong>Reproducibility:</strong> Tracking experiments and comparing results</p></li><li><p class=\"text-node\">\u23f1\ufe0f <strong>Time-consuming:</strong> Weeks of trial-and-error before large-scale training</p></li></ul><h2 class=\"heading-node\">\ud83d\udca1 How STIMULUS Solves This</h2><p class=\"text-node\">The pipeline automates the entire model development workflow:</p><ol class=\"list-node\"><li><p class=\"text-node\"><strong>\ud83d\udd04 Data Transformation:</strong> Systematically applies all possible preprocessing pipelines to your data</p><ul class=\"list-node\"><li><p class=\"text-node\">Normalization strategies (z-score, min-max, quantile)</p></li><li><p class=\"text-node\">Feature selection and filtering</p></li><li><p class=\"text-node\">Augmentation techniques</p></li><li><p class=\"text-node\">Dimensionality reduction</p></li></ul></li><li><p class=\"text-node\"><strong>\ud83c\udfd7\ufe0f Architecture Search:</strong> Finds optimal hyperparameters for each transformed dataset</p><ul class=\"list-node\"><li><p class=\"text-node\">Neural network depth and width</p></li><li><p class=\"text-node\">Learning rates and batch sizes</p></li><li><p class=\"text-node\">Dropout and regularization</p></li><li><p class=\"text-node\">Activation functions</p></li></ul></li><li><p class=\"text-node\"><strong>\u2705 Sanity Checks:</strong> Validates models to ensure they're learning meaningful patterns</p><ul class=\"list-node\"><li><p class=\"text-node\">Overfitting detection</p></li><li><p class=\"text-node\">Gradient flow analysis</p></li><li><p class=\"text-node\">Loss convergence verification</p></li></ul></li><li><p class=\"text-node\"><strong>\ud83c\udfc3 Minimal Training:</strong> Trains lightweight versions of each configuration</p><ul class=\"list-node\"><li><p class=\"text-node\">Quick performance evaluation</p></li><li><p class=\"text-node\">Cross-validation metrics</p></li><li><p class=\"text-node\">Statistical significance testing</p></li></ul></li><li><p class=\"text-node\"><strong>\ud83d\udcca Comprehensive Report:</strong> Compiles intuitive visualizations and rankings</p><ul class=\"list-node\"><li><p class=\"text-node\">Best preprocessing-architecture combinations</p></li><li><p class=\"text-node\">Performance comparisons</p></li><li><p class=\"text-node\">Recommendations for full-scale training</p></li></ul></li></ol><h2 class=\"heading-node\">\ud83d\ude80 Key Benefits</h2><p class=\"text-node\">Leveraging Nextflow's power (polyglotism, container integration, cloud scalability), this pipeline helps you:</p><ul class=\"list-node\"><li><p class=\"text-node\">\u26a1 <strong>Automate Testing:</strong> Run hundreds of experiments in parallel instead of manually</p></li><li><p class=\"text-node\">\ud83c\udfaf <strong>Gain Insights:</strong> Understand which data transformations and architectures work best for your task</p></li><li><p class=\"text-node\">\ud83c\udfce\ufe0f <strong>Accelerate Development:</strong> Reduce weeks of manual work to hours of automated testing</p></li><li><p class=\"text-node\">\ud83d\udcc8 <strong>Improve Performance:</strong> Find the optimal configuration before investing in full-scale training</p></li><li><p class=\"text-node\">\ud83d\udd2c <strong>Ensure Reproducibility:</strong> Containerized, version-controlled experiments with complete provenance</p></li></ul><h2 class=\"heading-node\">\ud83d\udccb Input Data Format</h2><p class=\"text-node\">Your data must be provided as a <strong>CSV file</strong> with special header formatting:</p><pre class=\"block-node\"><code>gene_1:input:float,gene_2:input:float,age:meta:int,disease_status:label:binary\n5.2,3.1,45,1\n4.8,2.9,52,0\n6.1,4.2,38,1</code></pre><p class=\"text-node\"><strong>Header Format:</strong> <code class=\"inline\" spellcheck=\"false\">column_name:type:class</code></p><ul class=\"list-node\"><li><p class=\"text-node\"><strong>input:</strong> Features fed into the model (gene expression, clinical values, etc.)</p></li><li><p class=\"text-node\"><strong>meta:</strong> Metadata tracked but not transformed or used for training</p></li><li><p class=\"text-node\"><strong>label:</strong> Target variable for supervised learning (classification or regression)</p></li></ul><h2 class=\"heading-node\">\ud83d\udd27 Custom Model Integration</h2><p class=\"text-node\">The pipeline supports <strong>custom PyTorch models</strong> with specific structural requirements:</p><ul class=\"list-node\"><li><p class=\"text-node\">PyTorch-based neural network definitions</p></li><li><p class=\"text-node\">Compatible with standard training loops</p></li><li><p class=\"text-node\">Configurable architecture parameters</p></li><li><p class=\"text-node\">Support for various loss functions and optimizers</p></li></ul><h2 class=\"heading-node\">\ud83c\udfaf Scientific Applications</h2><h3 class=\"heading-node\">Genomics & Bioinformatics</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>\ud83e\uddec Gene Expression Analysis:</strong> Find optimal normalization for RNA-seq classification</p></li><li><p class=\"text-node\"><strong>\ud83d\udd2c Variant Calling:</strong> Optimize feature engineering for genomic variant prediction</p></li><li><p class=\"text-node\"><strong>\ud83d\udcca Single-Cell:</strong> Identify best preprocessing for cell type classification</p></li><li><p class=\"text-node\"><strong>\ud83e\uddeb Proteomics:</strong> Test transformations for protein abundance prediction</p></li></ul><h3 class=\"heading-node\">Clinical & Translational Research</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>\ud83c\udfe5 Disease Prediction:</strong> Optimize multi-omics integration for diagnosis</p></li><li><p class=\"text-node\"><strong>\ud83d\udc8a Drug Response:</strong> Find best model for treatment outcome prediction</p></li><li><p class=\"text-node\"><strong>\ud83d\udd0d Biomarker Discovery:</strong> Test feature selection strategies</p></li><li><p class=\"text-node\"><strong>\ud83e\udde0 Patient Stratification:</strong> Optimize clustering and classification</p></li></ul><h3 class=\"heading-node\">Model Development</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>\ud83c\udf93 Proof of Concept:</strong> Quickly test if deep learning is appropriate for your data</p></li><li><p class=\"text-node\"><strong>\ud83d\udd2c Hypothesis Testing:</strong> Compare data transformation hypotheses systematically</p></li><li><p class=\"text-node\"><strong>\ud83d\udcc8 Baseline Establishment:</strong> Set performance benchmarks before production models</p></li><li><p class=\"text-node\"><strong>\u2699\ufe0f Pipeline Design:</strong> Determine optimal preprocessing for production pipelines</p></li></ul><h2 class=\"heading-node\">\ud83d\udcca Comprehensive Outputs</h2><h3 class=\"heading-node\">Experiment Results</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>\ud83d\udcc8 Performance Metrics:</strong> Accuracy, precision, recall, F1, AUC-ROC for all configurations</p></li><li><p class=\"text-node\"><strong>\ud83d\udcca Comparison Tables:</strong> Ranked preprocessing-architecture combinations</p></li><li><p class=\"text-node\"><strong>\ud83d\udcc9 Learning Curves:</strong> Training/validation loss plots for each experiment</p></li><li><p class=\"text-node\"><strong>\ud83c\udfaf Best Models:</strong> Top-performing configurations identified</p></li></ul><h3 class=\"heading-node\">Quality Control</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>\u2705 Sanity Check Reports:</strong> Validation that models are learning appropriately</p></li><li><p class=\"text-node\"><strong>\u26a0\ufe0f Failure Analysis:</strong> Identification of problematic configurations</p></li><li><p class=\"text-node\"><strong>\ud83d\udcca Statistical Tests:</strong> Significance of performance differences</p></li></ul><h3 class=\"heading-node\">Interactive Reports</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>\ud83d\udcf1 HTML Dashboard:</strong> Interactive exploration of all experiments</p></li><li><p class=\"text-node\"><strong>\ud83d\udcca Visualization Suite:</strong> Heatmaps, scatter plots, box plots comparing methods</p></li><li><p class=\"text-node\"><strong>\ud83d\udca1 Recommendations:</strong> Clear guidance on which configuration to use for production</p></li></ul><h3 class=\"heading-node\">Trained Models</h3><ul class=\"list-node\"><li><p class=\"text-node\"><strong>\ud83d\udcbe Model Checkpoints:</strong> Saved weights for best-performing configurations</p></li><li><p class=\"text-node\"><strong>\u2699\ufe0f Hyperparameters:</strong> Complete configuration files for reproduction</p></li><li><p class=\"text-node\"><strong>\ud83d\udccb Training Logs:</strong> Detailed execution traces for debugging</p></li></ul><h2 class=\"heading-node\">\ud83d\udd2c Workflow Philosophy</h2><blockquote><p class=\"text-node\"><strong>\"Most of the performance is driven by the training data\"</strong></p></blockquote><p class=\"text-node\">Rather than spending weeks manually testing preprocessing approaches, STIMULUS systematically explores the data transformation space. This ensures you're not leaving performance on the table due to suboptimal data preparation, which is often the biggest bottleneck in deep learning success.</p><h2 class=\"heading-node\">\u26a1 Powered by Nextflow & nf-core</h2><ul class=\"list-node\"><li><p class=\"text-node\"><strong>\ud83d\udd04 Reproducible:</strong> Containerized execution (Docker/Singularity)</p></li><li><p class=\"text-node\"><strong>\u2601\ufe0f Scalable:</strong> Run locally or on cloud (AWS, GCP, Azure)</p></li><li><p class=\"text-node\"><strong>\ud83d\udcca Portable:</strong> Works on HPC clusters, laptops, or cloud platforms</p></li><li><p class=\"text-node\"><strong>\u2705 Tested:</strong> Continuous integration with comprehensive test suite</p></li><li><p class=\"text-node\"><strong>\ud83d\udcda Documented:</strong> Extensive documentation and community support</p></li></ul><p class=\"text-node\"><strong>Perfect for:</strong> Machine learning model development \u2022 Deep learning optimization \u2022 Genomics data analysis \u2022 Clinical prediction models \u2022 Bioinformatics research \u2022 Data science workflows \u2022 Automated hyperparameter tuning \u2022 Model comparison studies</p><p class=\"text-node\">Built with <a class=\"link\" href=\"https://www.nextflow.io/\" target=\"_blank\">Nextflow</a> \u2022 Powered by <a class=\"link\" href=\"https://nf-co.re/deepmodeloptim/dev/\" target=\"_blank\">nf-core/deepmodeloptim</a> \u2022 Containerized execution \u2022 Open source</p>",
  "spec": [
    {
      "type": "Stash File",
      "label": "Training Data (CSV)",
      "name": "input",
      "description": "CSV file with special header format: column_name:type:class where type is 'input', 'meta', or 'label'. Input columns are features for training, meta columns are tracked metadata, label is the target variable.",
      "defaultValue": "",
      "hidden": false,
      "required": true,
      "disabled": false,
      "restrictions": {
        "allow_files": true,
        "allow_folders": false,
        "allowed_file_types": [
          ".csv"
        ]
      }
    },
    {
      "type": "Stash File",
      "label": "Output Directory",
      "name": "outdir",
      "description": "Directory for all pipeline outputs including trained models, performance metrics, comparison reports, and visualizations.",
      "defaultValue": "./deepmodeloptim-results",
      "hidden": false,
      "required": true,
      "disabled": false,
      "restrictions": {
        "allow_files": false,
        "allow_folders": true
      }
    },
    {
      "type": "Select",
      "label": "Optimization Strategy",
      "name": "optimization_mode",
      "description": "Level of preprocessing exploration: Quick (few transformations), Standard (balanced), Comprehensive (exhaustive testing).",
      "defaultValue": "standard",
      "hidden": false,
      "required": false,
      "disabled": false,
      "options": [
        {
          "label": "Quick - Fast exploration with common preprocessing",
          "value": "quick"
        },
        {
          "label": "Standard - Balanced testing (Recommended)",
          "value": "standard"
        },
        {
          "label": "Comprehensive - Exhaustive preprocessing combinations",
          "value": "comprehensive"
        }
      ]
    },
    {
      "type": "Select",
      "label": "Task Type",
      "name": "task_type",
      "description": "Type of machine learning task: binary classification, multi-class classification, or regression.",
      "defaultValue": "binary_classification",
      "hidden": false,
      "required": true,
      "disabled": false,
      "options": [
        {
          "label": "Binary Classification (0/1, True/False)",
          "value": "binary_classification"
        },
        {
          "label": "Multi-class Classification (3+ categories)",
          "value": "multiclass_classification"
        },
        {
          "label": "Regression (Continuous values)",
          "value": "regression"
        }
      ]
    },
    {
      "type": "Input",
      "label": "Cross-Validation Folds",
      "name": "cv_folds",
      "description": "Number of cross-validation folds for model evaluation (typically 5 or 10).",
      "defaultValue": "5",
      "hidden": false,
      "required": false,
      "disabled": false
    }
  ],
  "jobConfig": [
    {
      "type": "Select",
      "label": "Compute Resources",
      "name": "system_size",
      "description": "Resource allocation based on dataset size and complexity. GPU support available for large-scale experiments.",
      "hidden": false,
      "options": [
        {
          "label": "MEDIUM (32 CPUs, 180GB RAM) - Small datasets (<10K samples)",
          "value": "medium",
          "mapValue": {
            "nodeSize": "MEDIUM",
            "numNodes": 1,
            "withGpu": false
          }
        },
        {
          "label": "LARGE (64 CPUs, 360GB RAM) - Recommended for most experiments",
          "value": "large",
          "mapValue": {
            "nodeSize": "LARGE",
            "numNodes": 1,
            "withGpu": false
          }
        },
        {
          "label": "XLARGE (96 CPUs, 540GB RAM) - Large datasets (>50K samples)",
          "value": "xlarge",
          "mapValue": {
            "nodeSize": "XLARGE",
            "numNodes": 1,
            "withGpu": false
          }
        },
        {
          "label": "LARGE + GPU - Deep learning with GPU acceleration",
          "value": "large_gpu",
          "mapValue": {
            "nodeSize": "LARGE",
            "numNodes": 1,
            "withGpu": true
          }
        }
      ],
      "defaultValue": "large"
    }
  ],
  "tags": [
    {
      "name": "machine-learning",
      "type": "field"
    },
    {
      "name": "deep-learning",
      "type": "subfield"
    },
    {
      "name": "optimization",
      "type": "task"
    },
    {
      "name": "model-training",
      "type": "task"
    },
    {
      "name": "genomics",
      "type": "field"
    },
    {
      "name": "bioinformatics",
      "type": "field"
    }
  ]
}